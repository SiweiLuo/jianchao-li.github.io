<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Jianchao Li">

  
  
  
    
  
  <meta name="description" content="Recently I was working with PyTorch multi-GPU training and I came across a nightmare GPU memory problem. After some expensive trial and error, I finally found a solution for it.">

  
  <link rel="alternate" hreflang="en-us" href="https://jianchao-li.github.io/post/killing-pytorch-multi-gpu-training-the-safe-way/">

  


  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.747d92dc45be93ba6055a623fe5bcc0a.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-149941944-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://jianchao-li.github.io/post/killing-pytorch-multi-gpu-training-the-safe-way/">

  
  
  
  
    
    
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Jianchao Li">
  <meta property="og:url" content="https://jianchao-li.github.io/post/killing-pytorch-multi-gpu-training-the-safe-way/">
  <meta property="og:title" content="Killing Pytorch Multi Gpu Training the Safe Way | Jianchao Li">
  <meta property="og:description" content="Recently I was working with PyTorch multi-GPU training and I came across a nightmare GPU memory problem. After some expensive trial and error, I finally found a solution for it."><meta property="og:image" content="https://jianchao-li.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-11-02T14:58:10&#43;08:00">
  
  <meta property="article:modified_time" content="2018-11-02T14:58:10&#43;08:00">
  

  


  





  <title>Killing Pytorch Multi Gpu Training the Safe Way | Jianchao Li</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" role="textbox" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Jianchao Li</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience">
            
            <span>Experience</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#accomplishments">
            
            <span>Accomplishments</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Killing Pytorch Multi Gpu Training the Safe Way</h1>

  

  
    



<meta content="2018-11-02 14:58:10 &#43;0800 &#43;08" itemprop="datePublished">
<meta content="2018-11-02 14:58:10 &#43;0800 &#43;08" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    <time>Nov 2, 2018</time>
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="/post/killing-pytorch-multi-gpu-training-the-safe-way/#disqus_thread"></a>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Killing%20Pytorch%20Multi%20Gpu%20Training%20the%20Safe%20Way&amp;url=https%3a%2f%2fjianchao-li.github.io%2fpost%2fkilling-pytorch-multi-gpu-training-the-safe-way%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fjianchao-li.github.io%2fpost%2fkilling-pytorch-multi-gpu-training-the-safe-way%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjianchao-li.github.io%2fpost%2fkilling-pytorch-multi-gpu-training-the-safe-way%2f&amp;title=Killing%20Pytorch%20Multi%20Gpu%20Training%20the%20Safe%20Way"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fjianchao-li.github.io%2fpost%2fkilling-pytorch-multi-gpu-training-the-safe-way%2f&amp;title=Killing%20Pytorch%20Multi%20Gpu%20Training%20the%20Safe%20Way"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Killing%20Pytorch%20Multi%20Gpu%20Training%20the%20Safe%20Way&amp;body=https%3a%2f%2fjianchao-li.github.io%2fpost%2fkilling-pytorch-multi-gpu-training-the-safe-way%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<p>As you may have noticed from the title, this post is somewhat different from my previous ones. I would like to talk about a PyTorch DataLoader issue I encountered recently. I feel like devoting a post to it because it has taken me long time to figure out how to fix it.</p>

<h2 id="memory-consumption">Memory consumption</h2>

<p>Since my last post on <a href="https://jianchao-li.github.io/2018/09/15/understanding-fully-convolutional-networks/" target="_blank">FCNs</a>, I have been working on semantic segmentation. Nowadays, we have deep neural networks for it, like the state-of-the-art <a href="https://arxiv.org/abs/1612.01105" target="_blank">PSPNet</a> from CVPR 2017.</p>

<p>In practice, segmentation networks are <strong>much more memory-intensive</strong> than recognition/classification networks. The reason is that semantic segmentation requires dense pixel-level predictions. For example, in the ImageNet classification task, you may use a neural network to transform a 224x224 image into 1000 real numbers (class probabilities). However, in semantic segmentation, suppose you have 20 semantic classes, you need to transform the 224x224 image into 20 224x224 probability maps, each representing probabilities of pixels belonging to one class. The output size changes from 1000 to 20x224x224=1003520, which is more than 1000 times!</p>

<p>Besides the output, the intermediate feature maps in segmentation networks also consume more memory. In recognition networks, sizes of intermediate feature maps usually decrease monotically. However, since segmentation requires output of the same spatial dimension as the input, the feature maps will go through an extra process with their sizes increased (upsampled) back to the size of the input image. This extra upsample process further increases the memory consumption of segmentation networks.</p>

<p>So, when we fit segmentation networks on a GPU, we need to reduce the batch size of the data. However, batch size is crucial to the performance of networks, especially those containing the batch normalization layer. Since no more data can be held in a single GPU, a natural soltuion is to use multiple GPUs and split the data across them (or more formally, <em>data parallelism</em>).</p>

<h2 id="synchronized-batch-normalization">Synchronized batch normalization</h2>

<p>Here I would like to make a digression and mention an interesting layer, the synchronized batch normalization layer, which is introduced to increase the <em>working batch size</em> for multi-GPU training. You may refer to the section <strong>Cross-GPU Batch Normalization</strong> in <a href="https://arxiv.org/abs/1711.07240" target="_blank">MegDet</a> for more details.</p>

<p>When we use data parallelism to train on multiple GPUs, a batch of images will be splitted across several GPUs. Suppose your batch size is 16 (a common setting in semantic segmentation) and you train on 8 GPUs with data parallelism, then each GPU will have 2 images. A normal batch norm layer will only uses the 2 images on a single GPU to compute the mean and standard deviation, which is highly inaccurate and will make the training unstable.</p>

<p>To effectively increase the working batch size, we need to synchronize all the GPUs in the batch norm layer, and fetch the mean and standard deviation computed at each GPU to compute a global value using all images. This is what synchronized batch norm layer does. If you would like to learn more about its implementation details, you may have a look at <a href="https://github.com/vacancy/Synchronized-BatchNorm-PyTorch" target="_blank">Synchronized-BatchNorm-PyTorch</a>.</p>

<h2 id="the-issue">The Issue</h2>

<p>After so much background information, the main idea is that semantic segmentation networks are very memory-intensive and require multiple GPUs to train a reasonable batch size. And synchronized batch norm can be used to increase the working batch size in multi-GPU training.</p>

<p>Now comes the issue that I encountered recently. I was working with a semantic segmentation codebase written in PyTorch on a machine with 8 GPUs. The codebase incorporates synchronized batch norm and uses PyTorch multiprocessing for its custom DataLoader. I ran the training program for some time and then I killed it (I was running the program in a virtualized docker container in a cloud GPU cluster. So killing it is just to click a button in the cloud GUI).</p>

<p>Then I checked the GPUs using <code>nvidia-smi</code> and everything looked good.</p>

<pre><code class="language-bash">+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.46                 Driver Version: 390.46                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  Off  | 00000000:1A:00.0 Off |                    0 |
| N/A   32C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  Off  | 00000000:1F:00.0 Off |                    0 |
| N/A   34C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  Off  | 00000000:20:00.0 Off |                    0 |
| N/A   33C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  Off  | 00000000:21:00.0 Off |                    0 |
| N/A   33C    P0    23W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  Off  | 00000000:B2:00.0 Off |                    0 |
| N/A   32C    P0    26W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  Tesla V100-PCIE...  Off  | 00000000:B3:00.0 Off |                    0 |
| N/A   35C    P0    26W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  Tesla V100-PCIE...  Off  | 00000000:B4:00.0 Off |                    0 |
| N/A   34C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  Tesla V100-PCIE...  Off  | 00000000:B5:00.0 Off |                    0 |
| N/A   35C    P0    25W / 250W |     11MiB / 16160MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>

<p>But then when I tried to start a new training program. An OOM error occurred. For the sake of privacy, some traceback logs were omitted by <code>...</code>.</p>

<pre><code class="language-bash">Traceback (most recent call last):
  ...
  File &quot;/usr/local/lib/python3.6/site-packages/torch/cuda/streams.py&quot;, line 21, in __new__
    return super(Stream, cls).__new__(cls, priority=priority, **kwargs)
RuntimeError: CUDA error (2): out of memory
Exception in thread Thread-1:
Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.6/threading.py&quot;, line 916, in _bootstrap_inner
    self.run()
  File &quot;/usr/local/lib/python3.6/threading.py&quot;, line 864, in run
    self._target(*self._args, **self._kwargs)
  ...
  File &quot;/usr/local/lib/python3.6/multiprocessing/queues.py&quot;, line 337, in get
    return _ForkingPickler.loads(res)
  File &quot;/usr/local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py&quot;, line 151, in rebuild_storage_fd
    fd = df.detach()
  File &quot;/usr/local/lib/python3.6/multiprocessing/resource_sharer.py&quot;, line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File &quot;/usr/local/lib/python3.6/multiprocessing/resource_sharer.py&quot;, line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File &quot;/usr/local/lib/python3.6/multiprocessing/connection.py&quot;, line 493, in Client
    answer_challenge(c, authkey)
  File &quot;/usr/local/lib/python3.6/multiprocessing/connection.py&quot;, line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File &quot;/usr/local/lib/python3.6/multiprocessing/connection.py&quot;, line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File &quot;/usr/local/lib/python3.6/multiprocessing/connection.py&quot;, line 407, in _recv_bytes
    buf = self._recv(4)
  File &quot;/usr/local/lib/python3.6/multiprocessing/connection.py&quot;, line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
</code></pre>

<p>I ran <code>nvidia-smi</code> again and everything still seemed good. So I wrote a <code>check.cu</code> to check the GPU memory using CUDA APIs.</p>

<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &quot;cuda.h&quot;
#include &quot;cuda_runtime_api.h&quot;
  
using namespace std;
  
int main( void ) {
    int num_gpus;
    size_t free, total;
    cudaGetDeviceCount( &amp;num_gpus );
    for ( int gpu_id = 0; gpu_id &lt; num_gpus; gpu_id++ ) {
        cudaSetDevice( gpu_id );
        int id;
        cudaGetDevice( &amp;id );
        cudaMemGetInfo( &amp;free, &amp;total );
        cout &lt;&lt; &quot;GPU &quot; &lt;&lt; id &lt;&lt; &quot; memory: free=&quot; &lt;&lt; free &lt;&lt; &quot;, total=&quot; &lt;&lt; total &lt;&lt; endl;
    }
    return 0;
}
</code></pre>

<p>Again, everything looked good.</p>

<pre><code class="language-bash">$ nvcc check.cu -o check &amp;&amp; ./check
GPU 0 memory: free=16488464384, total=16945512448
GPU 1 memory: free=16488464384, total=16945512448
GPU 2 memory: free=16488464384, total=16945512448
GPU 3 memory: free=16488464384, total=16945512448
GPU 4 memory: free=16488464384, total=16945512448
GPU 5 memory: free=16488464384, total=16945512448
GPU 6 memory: free=16488464384, total=16945512448
GPU 7 memory: free=16488464384, total=16945512448
</code></pre>

<p>Since the error happened to PyTorch, I moved on to write a <code>check.py</code>, which created a single-element PyTorch CUDA tensor for sanity check. And this script reproduced the OOM error.</p>

<pre><code class="language-python">import torch
 
if __name__ == '__main__':
    num_gpus = torch.cuda.device_count()
    for gpu_id in range(num_gpus):
        try:
	    torch.cuda.set_device(gpu_id)
	    torch.randn(1, device='cuda')
	    print('GPU {} is good'.format(gpu_id))
        except Exception as exec:
            print('GPU {} is bad: {}'.format(gpu_id, exec))
</code></pre>

<p>The output was as follows: GPU 1 and 2 were OOM.</p>

<pre><code class="language-bash">$ python3 check.py 
GPU 0 is good
GPU 1 is bad: CUDA error: out of memory
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=663 error=2 : out of memory
GPU 2 is bad: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/THCGeneral.cpp:663
GPU 3 is good
GPU 4 is good
GPU 5 is good
GPU 6 is good
GPU 7 is good
</code></pre>

<p>So, my GPUs 2 and 3 should be magically occupied by some zombie process. And I had to restart the machine to fix it. I think the zombie process was generated due to my incorrect way of killing the training program. So I decided not to use the kill button in the cloud GUI but logged into the docker container to kill it in the terminal.</p>

<p>I searched on Google for how to kill a PyTorch multi-GPU training program. And I found <a href="https://discuss.pytorch.org/u/smth/summary" target="_blank">@smth</a>&rsquo;s suggestion in <a href="https://discuss.pytorch.org/t/pytorch-doesnt-free-gpus-memory-of-it-gets-aborted-due-to-out-of-memory-error/13775/14?u=jianchao-li" target="_blank">this reply</a>.</p>

<blockquote>
<p>@rabst
so, I remember this issue. When investigating, we found that thereâ€™s actually a bug in python multiprocessing that might keep the child process hanging around, as zombie processes.
It is not even visible to <code>nvidia-smi</code> .
The solution is <code>killall python</code> , or to <code>ps -elf | grep python</code> and find them and <code>kill -9 [pid]</code> to them.</p>
</blockquote>

<p>It explained why <code>nvidia-smi</code> failed to reveal the memory issue. Great! But, the above commands did not work for me&hellip;</p>

<blockquote>
<p>Nothing is so fatiguing as the eternal haning on of an uncompleted task.
<div style="text-align: right;">&mdash; William James</div></p>
</blockquote>

<h2 id="the-solution">The Solution</h2>

<p>After several days of searching, failing, searching again, failing again etc., I finally found one solution. It is just to find out the processes that occupied the GPUs and kill them. To find out those processes, I ran <code>fuser -v /dev/nvidia*</code>, which listed all the processes that were occupying my NVIDIA GPUs. Since I have 8 GPUs, the output of this command is a bit log.</p>

<pre><code class="language-bash">$ fuser -v /dev/nvidia*
                     USER        PID ACCESS COMMAND
/dev/nvidia0:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia1:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia2:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia3:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia4:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia5:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia6:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia7:        root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidiactl:      root       5284 F...m python3
                     root       5416 F...m python3
                     root       5417 F...m python3
                     root       5418 F...m python3
                     root       5419 F...m python3
                     root       5420 F...m python3
                     root       5421 F...m python3
                     root       5422 F...m python3
                     root       5423 F...m python3
                     root       5424 F...m python3
                     root       5425 F...m python3
                     root       5426 F...m python3
                     root       5427 F...m python3
                     root       5428 F...m python3
                     root       5429 F...m python3
                     root       5430 F...m python3
                     root       5431 F...m python3
/dev/nvidia-uvm:     root       5284 F.... python3
                     root       5416 F.... python3
                     root       5417 F.... python3
                     root       5418 F.... python3
                     root       5419 F.... python3
                     root       5420 F.... python3
                     root       5421 F.... python3
                     root       5422 F.... python3
                     root       5423 F.... python3
                     root       5424 F.... python3
                     root       5425 F.... python3
                     root       5426 F.... python3
                     root       5427 F.... python3
                     root       5428 F.... python3
                     root       5429 F.... python3
                     root       5430 F.... python3
                     root       5431 F.... python3
</code></pre>

<p>As can be seen from above, the main process had a PID of 5284. I spawned 16 workers for the DataLoader so there were 16 subprocesses whose PIDs were consecutive (from 5416 to 5431). First I used <code>kill -9</code> to kill all of them. Then killed the main process.</p>

<pre><code class="language-bash">$ for pid in {5416..5431}; do kill -9 $pid; done # kill subprocesses
$ kill -9 5284 # kill main process
</code></pre>

<p>After killing the subprocesses and main process, I ran <code>check.py</code> again and this time every GPU was good.</p>

<pre><code class="language-bash">$ python3 check.py 
GPU 0 is good
GPU 1 is good
GPU 2 is good
GPU 3 is good
GPU 4 is good
GPU 5 is good
GPU 6 is good
GPU 7 is good
</code></pre>

<h2 id="another-trick">Another Trick</h2>

<p>If the above solution still does not work for you (it does happen to me sometimes), the following trick may be helpful. First, find out the training loop of your program. In most cases it will contain a loop based on the number of iterations. Then add the following code to that loop.</p>

<pre><code class="language-python">if os.path.isfile('kill.me'):
    num_gpus = torch.cuda.device_count()
    for gpu_id in range(num_gpus):
        torch.cuda.set_device(gpu_id)
        torch.cuda.empty_cache()
    exit(0)
</code></pre>

<p>Inside the <code>if</code> statement, the code empties the caches of all GPUs and exits. After you add this code to the training iteration, once you want to stop it, just <code>cd</code> into the directory of the training program and run</p>

<pre><code class="language-bash">touch kill.me
</code></pre>

<p>Then in the current or next iteration (based on whether the above code has been executed), the <code>if</code> check will become true and all GPUs will be cleared and the program will exit. Since you directly tell Python to exit in the program, it will take care of everything for you. You may use anything instead of <code>kill.me</code>. But just make sure it is special enough and thus you will not terminate the training inadvertently by creating a file with the same name.</p>

<h2 id="conclusions">Conclusions</h2>

<p>The issue made me stuck for long time. And in this process of looking for the solution, I made some expensive trial and error. Several GPU servers in the cloud had a card OOM due to my incorrect way of killing the training program. And I had to ask the administrators to restart them. So I would definitely like others to avoid such a case.</p>

<p>From another perspective, I would like to highlight the importance of engineering capabilities and experiences. Though I was working on semantic segmentation, I spent most of my time digging through all sorts of problems while running the multi-GPU codes.</p>

<p>A final remark, I would not like to leave you an impression that I am blaming the issue on PyTorch, CUDA, the cloud GPU cluster, or any others. Actually it is mainly due to that I do not understand how PyTorch multi-GPU and multiprocessing work. And I think I will need to study these topics more systematically. Hopefully I will write a new post after learning more about them :-)</p>

    </div>

    



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hufb61b7379fe204f22c406fec86aafd04_234902_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://jianchao-li.github.io">Jianchao Li</a></h5>
      <h6 class="card-subtitle">Software Engineer</h6>
      <p class="card-text" itemprop="description">I love writing deep learning codes for computer vision.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/jianchao-li" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/jianchao-li/" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.com.sg/citations?user=smH7fsYAAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "jianchaoli" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
      

      
      
    

    
    

    
    
    
    <script id="dsq-count-scr" src="//jianchaoli.disqus.com/count.js" async></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.e5372221df876d5d52eda40edaa58012.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2019 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
